{"componentChunkName":"component---node-modules-gatsby-theme-portfolio-minimal-src-templates-article-listing-index-tsx","path":"/blog/","result":{"pageContext":{"articles":[{"banner":{"alt":"","caption":"","src":null},"body":"<h2>Beyond the Prompt: Engineering Smarter AI with Semantic Kernel</h2>\n<p>As the adoption of Large Language Models (LLMs) grows, developers need a structured way to orchestrate prompts, manage AI skills, and integrate AI capabilities into software systems. This is where <strong>Semantic Kernel</strong> comes in - a powerful SDK from Microsoft that helps developers build AI-first apps using semantic functions, native code, and LLMs like OpenAI or Azure OpenAI.</p>\n<p>In this blog, we'll explore what Semantic Kernel is, how it works, and how you can use it to build scalable, modular, and context aware AI applications.</p>\n<hr>\n<h2>What is Semantic Kernel?</h2>\n<p><strong>Semantic Kernel (SK)</strong> is an <strong>open-source SDK</strong> that allows you to integrate <strong>AI services (like OpenAI GPT models)</strong> with <strong>traditional programming constructs</strong> such as functions, plugins, memory, and workflows.</p>\n<p>It is available in:</p>\n<ul>\n<li>C#</li>\n<li>Python</li>\n<li>Java (preview)</li>\n</ul>\n<p>With SK, you can combine <strong>semantic functions</strong> (prompt templates) and <strong>native functions</strong> (code-based logic) in one seamless pipeline—enabling <strong>hybrid AI systems</strong>.</p>\n<hr>\n<h2>Key Features</h2>\n<ul>\n<li><strong>Semantic functions</strong>: Use prompt engineering as first-class components</li>\n<li><strong>Pluggable AI models</strong>: Support for OpenAI, Azure OpenAI, Hugging Face, and more</li>\n<li><strong>Memory store</strong>: Persistent vector memory for context-aware reasoning</li>\n<li><strong>Planner</strong>: Auto-generates plans (sequences of steps) using AI</li>\n<li><strong>Skills architecture</strong>: Organize functions into reusable modules</li>\n</ul>\n<hr>\n<h2>How It Works</h2>\n<p>Here’s a simplified flow of a Semantic Kernel application:</p>\n<div class=\"gatsby-highlight\" data-language=\"plaintext\"><pre class=\"language-plaintext\"><code class=\"language-plaintext\">User Request\n     ↓\nSemantic Kernel\n     ↓\n[Semantic Functions + Native Functions + Memory + Planner]\n     ↓\nExternal LLM APIs (OpenAI, Azure OpenAI)\n     ↓\nResponse</code></pre></div>\n<p>You can define prompts (semantic functions), call native APIs (C#/Python code), retrieve memory from vector stores, and even generate plans automatically.</p>\n<h2>Installing Semantic Kernel (Python)</h2>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">pip install semantic<span class=\"token operator\">-</span>kernel</code></pre></div>\n<h2>Example: Build a Chatbot with Semantic Kernel</h2>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">import</span> semantic_kernel <span class=\"token keyword\">as</span> sk\n<span class=\"token keyword\">from</span> semantic_kernel<span class=\"token punctuation\">.</span>connectors<span class=\"token punctuation\">.</span>ai<span class=\"token punctuation\">.</span>open_ai <span class=\"token keyword\">import</span> OpenAIChatCompletion\n\n<span class=\"token comment\"># Create kernel</span>\nkernel <span class=\"token operator\">=</span> sk<span class=\"token punctuation\">.</span>Kernel<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nkernel<span class=\"token punctuation\">.</span>add_chat_service<span class=\"token punctuation\">(</span>\n    <span class=\"token string\">\"chat-gpt\"</span><span class=\"token punctuation\">,</span>\n    OpenAIChatCompletion<span class=\"token punctuation\">(</span><span class=\"token string\">\"gpt-4\"</span><span class=\"token punctuation\">,</span> api_key<span class=\"token operator\">=</span><span class=\"token string\">\"YOUR_OPENAI_API_KEY\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Create a semantic function from a prompt</span>\nprompt_template <span class=\"token operator\">=</span> <span class=\"token string\">\"You're a helpful assistant. Answer: {{$input}}\"</span>\nchat_function <span class=\"token operator\">=</span> kernel<span class=\"token punctuation\">.</span>create_semantic_function<span class=\"token punctuation\">(</span>prompt_template<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Run the function</span>\noutput <span class=\"token operator\">=</span> chat_function<span class=\"token punctuation\">(</span><span class=\"token string\">\"What is Semantic Kernel?\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Using Semantic Functions and Native Functions Together</h2>\n<p>You can define a semantic skill using a .txt prompt file and then call it from Python or C#. Native functions (like API calls or math logic) can be registered as well.</p>\n<p>Example: Mixing prompts and Python logic:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_user_name</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">></span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">\"Rahul\"</span>\n\nkernel<span class=\"token punctuation\">.</span>register_python_function<span class=\"token punctuation\">(</span>get_user_name<span class=\"token punctuation\">,</span> <span class=\"token string\">\"user_skill\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"get_user_name\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Adding Memory (Vector Store Integration)</h2>\n<p>Semantic Kernel supports:</p>\n<ul>\n<li>Volatile memory (in-memory)</li>\n<li>Persistent memory with plugins like:\n<ol>\n<li>Azure Cognitive Search</li>\n<li>Pinecone</li>\n<li>Redis</li>\n</ol>\n</li>\n</ul>\n<p>Add memory to your kernel:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> semantic_kernel<span class=\"token punctuation\">.</span>memory<span class=\"token punctuation\">.</span>memory_store_base <span class=\"token keyword\">import</span> MemoryStoreBase\nkernel<span class=\"token punctuation\">.</span>register_memory_store<span class=\"token punctuation\">(</span>MyCustomVectorStore<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>then:</p>\n<ul>\n<li>Store summaries, documents, chat history</li>\n<li>Query memory for relevant context</li>\n<li>Automatically ground LLM responses with context</li>\n</ul>\n<h2>Using the Planner for Auto-Generated Workflows</h2>\n<p>The Planner uses LLMs to generate step-by-step workflows (plans) based on user intent.</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">planner <span class=\"token operator\">=</span> SequentialPlanner<span class=\"token punctuation\">(</span>kernel<span class=\"token punctuation\">)</span>\nplan <span class=\"token operator\">=</span> planner<span class=\"token punctuation\">.</span>create_plan<span class=\"token punctuation\">(</span><span class=\"token string\">\"Translate this text and summarize it in one line.\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>This is useful for:</p>\n<ul>\n<li>Task chaining</li>\n<li>Intelligent agents</li>\n<li>Zero-shot orchestration</li>\n</ul>\n<h2>Architecture Overview</h2>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">                      ┌────────────┐\n   ┌──────────────┐   │ Semantic   │\n   │ User Input   ├──►│  Kernel    ├──┐\n   └──────────────┘   └────┬───────┘  │\n                           ▼          ▼\n         ┌────────┐  ┌──────────┐ ┌──────────┐\n         │ Prompt │  │  Memory  │ │  Native  │\n         │Engine  │  │ (Vector) │ │ Function │\n         └────────┘  └──────────┘ └──────────┘\n                           ▼\n                   External LLM (e.g., GPT-4)</code></pre></div>\n<h2>Use Cases for Semantic Kernel</h2>\n<ul>\n<li><b>AI copilots</b>: Embed AI into productivity tools (e.g., Office, VS Code)</li>\n<li><b>Multi-step agents</b>: Automate goal-driven behavior with planning</li>\n<li><b>Conversational AI</b>: Build advanced chatbots with memory and context</li>\n<li><b>RAG systems</b>: Retrieve content and augment prompts dynamically</li>\n<li><b>Data enrichment</b>: Combine AI and native logic to tag, summarize, classify</li>\n</ul>\n<h2>Further deep-dive</h2>\n<ul>\n<li><u><a href=\"https://github.com/microsoft/semantic-kernel\">GitHub: Semantic Kernel</a></u></li>\n</ul>\n<hr>\nAuthor: <br/>\n<u><a href=\"https://www.linkedin.com/in/rahul-majumdar/\">Rahul Majumdar</a></u>","categories":["AI","Semantic Kernel"],"date":"February 10, 2025","description":"Beyond the Prompt: Engineering Smarter AI with Semantic Kernel","id":"bc3ae7a3-33f3-5742-918e-de6f5784ab7a","keywords":["Artificial Intelligence","System Design","System Architecture"],"slug":"/building-ai-applications-using-semantic-kernel/","title":"Hands-On AI Orchestration with Semantic Kernel","readingTime":{"text":"4 min read"}},{"banner":{"alt":"","caption":"","src":null},"body":"<!-- ## Code block test -->\n<!-- \n```css\n.AClass .Subtitle {\n  margin: -0.5rem 0 0 0;\n  font-weight: 700;\n  font-size: 1.25rem;\n  line-height: 1.5rem;\n}\n\n.AnotherClass p {\n  font-size: 1.125rem;\n  margin-bottom: 2rem;\n}\n\n.AThirdClass {\n  display: flex;\n  justify-content: flex-start;\n  align-items: center;\n}\n\n@media (max-width: 768px) {\n  .AClass {\n    flex-direction: column;\n  }\n  .AnotherClass {\n    display: block;\n  }\n}\n``` -->\n<style>\n.styled-table {\n  border-collapse: collapse;\n  margin: 25px 0;\n  font-size: 16px;\n  font-family: sans-serif;\n  min-width: 400px;\n  width: 100%;\n  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n}\n\n.styled-table thead tr {\n  background-color: #009879;\n  color: #ffffff;\n  text-align: left;\n}\n\n.styled-table th,\n.styled-table td {\n  padding: 12px 15px;\n  border: 1px solid #dddddd;\n}\n\n.styled-table tbody tr {\n  border-bottom: 1px solid #dddddd;\n}\n\n.styled-table tbody tr:nth-of-type(even) {\n  background-color: #f3f3f3;\n}\n\n.styled-table tbody tr:hover {\n  background-color: #f1f1f1;\n  cursor: pointer;\n}\n\n.gatsby-resp-image-wrapper {\n  display: inline !important;\n}\n\n</style>\n<p>Want to <u>design the system architecture of a video streaming platform like <b>YouTube</b></u>? Ever wondered what various components are needed and what it takes to create such a application at scale?</p>\n<h3>Specifications</h3>\n<p>To design such an application we will consider the below:</p>\n<ul>\n<li>Functional Requirements</li>\n<li>Non-functional Requirements</li>\n<li>Capacity planning (Back-of-envelope calculations)</li>\n<li>High-level design</li>\n<li>Deep dive</li>\n</ul>\n<h4>Functional Requirements</h4>\n<p>Below are the functional requirements that the system must address:</p>\n<ol>\n<li>Creator requirements: The creators/uploaders should be able to upload any video. The video needs to be made available for all locations. Latency for upload, processing and publishing is acceptable. Once published, the video should have high availability.</li>\n<li>Viewer requirements: Viewer should be able to view videos (high availability). Video should be compatible across multiple device types. Must be available for all network speeds. User should be able to search videos. User should have home feed (recommendation engine).</li>\n</ol>\n<h4>Non-functional Requirements</h4>\n<p>Let's make the following assumptions:</p>\n<ol>\n<li>We will assume we have 100M Daily Active Users (DAU's) using this application.</li>\n<li>Read:Write ratio is 100:1. Each user watches 100 videos per day while each creator uploads 1 video per day.</li>\n<li>Each video is 500MB in size.</li>\n<li>We would retain the data for 10 years.</li>\n<li>Video loading should have low latency.</li>\n<li>We should be able to scale globally and handle for more user spikes.</li>\n<li>System should have high availibility - should never be down for both users and creators.</li>\n</ol>\n<h4>Capacity Planning (Back-of-envelope calculations)</h4>\n<p>Let's calculate the QPS and storage for this system:</p>\n<h5>QPS (Queries per seconds)</h5>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">QPS = Total number of query requests per day / (Number of seconds in a day)</code></pre></div>\n<p>Assuming 100 million Daily Active Users (DAU), with each user performing one write operation per day and a Read-to-Write ratio of 100:1, the total daily read requests would be 100 million × 100 = 10 billion. This results in a Read QPS (Queries Per Second) of approximately 10 billion ÷ (24 × 3600) ≈ 115,740.</p>\n<h5>Storage</h5>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Storage = Total data volume per day * Number of days retained</code></pre></div>\n<p>Assuming each video is 500MB and there are 100 million write requests per day, the daily data volume amounts to 100 million × 500MB. To retain data for 10 years, the total storage needed is: 100 million × 500MB × 365 days × 10 years ≈ 183EB.</p>\n<h5>Bottlenecks</h5>\n<p>To identify bottlenecks, it is essential to analyze peak traffic periods and user distribution patterns. The primary bottleneck is likely to occur in video delivery, which can be addressed through the use of Content Delivery Networks (CDNs) and effective load balancing across the infrastructure.</p>\n<p>When estimating CDN costs, the primary consideration is content delivery. With a read-to-write ratio of 100:1, the majority of CDN expenses will stem from video streaming. However, implementing aggressive caching strategies and leveraging geographic distribution can significantly optimize these costs.</p>\n<h4>High Level Design</h4>\n<p>For this we will take a modular approach adopting microservices architecture of smaller interconnected services. This allows us for independent scaling, deployment and better fault isolation.</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 61.21212121212122%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAAsTAAALEwEAmpwYAAAB+0lEQVR42j1TW5aqMBBk/+uZO6PoGRQEFEEQkHcAcSF1u1qZjz6B0KmuR7Dm+QnWNM8YxwlVVaNuGkzTjK7rkOc57vf7p3J9Lx8PjGbE/HziKbUsi9QLy+sFaybQNOlHI02PqkJRlnjIobJ84HKJ4AchXO8E3w8Qns9Ikhv6ftAzCiqATwVdYBHEdb03gJRt7/DrHDAMRhiPOozPaZqiFcZVXaNtW1XAUoXKdNGyBmNwjWNtosStbePoun9AVBAnCb6/fxAEAfb7Pc7CkhZN0sNabWNZ3CBdNvTDgCy743ZLRVL/tkL27+LbZrvV9Xg84hJF6jXVrUx1gJTFIMjSiLy1yJSH86LA9Rqr3LpuBKRStm3boRFF9Jr9f6CTAr43+IESOblpWn1mqkEYIs0yZUwFBKHXDI/JcyhVrn5LKEYCqRRQJUsTi4zyvJCEA5xOJ3iS8sn3dWXKZMihZNzJsOlzWyx+cCRVJshg6NVutxfAWqXlZSG+ZojjRIFYZEyG7GegUXRVAlRh8eF8vqAoSjmYwxYw1/OErRE5pZYRnzmQTLqu10FcKfeWZqqIntMqy4zm48GkTak0JCKDQXx9/VPG9IzMQvGTzNQ/AYiFHe34dRwlxaEWfevk1nPl7V+n8wLzTv5sNjp5TZx31BMFVOEcDmoP/x56yV/2PxT6RJkb8OUfAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/9a5848ea74097f8437efc781f5cb91b5/c9b55/video_sd.webp 165w,\n/static/9a5848ea74097f8437efc781f5cb91b5/f2908/video_sd.webp 330w,\n/static/9a5848ea74097f8437efc781f5cb91b5/cc661/video_sd.webp 660w,\n/static/9a5848ea74097f8437efc781f5cb91b5/a66df/video_sd.webp 990w,\n/static/9a5848ea74097f8437efc781f5cb91b5/d33d4/video_sd.webp 1320w,\n/static/9a5848ea74097f8437efc781f5cb91b5/0abaa/video_sd.webp 1536w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/9a5848ea74097f8437efc781f5cb91b5/04c57/video_sd.png 165w,\n/static/9a5848ea74097f8437efc781f5cb91b5/d9ecf/video_sd.png 330w,\n/static/9a5848ea74097f8437efc781f5cb91b5/1f083/video_sd.png 660w,\n/static/9a5848ea74097f8437efc781f5cb91b5/7a3d6/video_sd.png 990w,\n/static/9a5848ea74097f8437efc781f5cb91b5/78958/video_sd.png 1320w,\n/static/9a5848ea74097f8437efc781f5cb91b5/71c1d/video_sd.png 1536w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/9a5848ea74097f8437efc781f5cb91b5/1f083/video_sd.png\" alt=\"high level design for youtube\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span>\n<p><b>When the user views the videos:</b>\nWhen a user requests to watch a video, the request flows through the <b>Load Balancer and API Gateway</b> to the <b>Video Playback Service</b>. This service first checks caching layers optimized for fast retrieval before querying the <b>Video Metadata Store</b> to obtain the video URL. Once the URL is fetched, the video is streamed directly from the nearest <b>CDN (Content Delivery Network)</b> node to the user's device, ensuring minimal latency and smooth playback.</p>\n<ul>\n<li>The <b>CDN</b> plays a critical role by delivering cached video content from geographically distributed nodes close to the user, significantly improving load times and overall viewing quality.</li>\n<li>The <b>Metadata Database</b> manages essential details such as video titles, descriptions, and user interactions (likes, comments, etc.). These databases are designed to handle large volumes of read operations efficiently.</li>\n</ul>\n<p><b>When the user uploads a video:</b>\nVideo uploads follow a separate flow. The process starts with the <b>Load Balancer and API Gateway</b> routing the upload request to the <b>Video Upload Service</b>.</p>\n<ul>\n<li>\n<p><b>Signed URL Generation</b>: The Video Upload Service requests a signed URL from the <b>Object Storage Service</b>, enabling secure, time-bound access to object storage platforms like <u><b>Amazon S3, Google Cloud Storage, or Azure Blob Storage</u></b>. The signed URL allows the client to upload files without burdening the application servers.</p>\n</li>\n<li>\n<p><b>Direct Upload</b>: The client application layer uses the signed URL to upload the video file directly to the object storage, bypassing the main application servers. This approach enhances scalability by reducing server load.</p>\n</li>\n<li>\n<p><b>Upload Confirmation &#x26; Metadata Submission</b>: Once the upload completes, the client notifies the <b>Video Upload Service</b> and provides relevant metadata, triggering the next set of operations.</p>\n</li>\n<li>\n<p><b>Video Processing Pipeline</b>: Uploaded videos undergo processing, including content moderation, transcoding (to support multiple formats and resolutions), compression, and thumbnail generation.</p>\n</li>\n<li>\n<p><b>CDN Distribution</b>: Finally, the processed video files are uploaded to CDN nodes, making them readily available to end users from the most optimal locations, ensuring fast and reliable playback.</p>\n</li>\n</ul>\n<h4>Deep dive - Low Level Design</h4>\n<p>Low-Level Design (LLD) delves into the detailed technical implementation of the system. It outlines how different components interact, specifies the data structures, class architectures, and defines the design of APIs.</p>\n<ol>\n<li><b>Service Modules</b></li>\n</ol>\n<ul>\n<li><b>Video Upload Service</b>: The Video Upload Service collaborates with the <b>Video Storage Service</b> and the <b>Transcoding Service</b>. Once a video is uploaded, it is stored as raw content in an object storage system (e.g., AWS S3). The Transcoding Service then processes the raw video into various formats and resolutions to ensure compatibility across devices. The upload service exposes endpoints such as <u>POST /upload-video</u> to handle incoming video files along with their metadata.</li>\n<li><b>Video Streaming Service</b>: Once the transcoding process is complete, the Video Streaming Service handles delivering the video to end users. It retrieves the video content from the distributed storage system and streams it in formats and resolutions optimized for the user's device and network conditions. Provides an endpoint such as <u>GET /video/{video_id}/stream</u>, which delivers video chunks for seamless streaming playback.</li>\n</ul>\n<ol start=\"2\">\n<li><b>Class Structure and Object-Oriented Design</b></li>\n</ol>\n<ul>\n<li><b>User Class</b>: Represents a user within the system, containing attributes such as user_id, username, email, password_hash, subscriptions, watch_history, and preferences. Key methods include:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">upload_video()\nlike_video()\nsubscribe_to_channel()\ncreate_playlist()</code></pre></div>\n<ul>\n<li><b>Video Class</b>: Represents a video entity with attributes like video_id, user_id (indicating the uploader), title, description, tags, views_count, upload_timestamp, and additional metadata. Key methods include:</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">get_video_info()\nincrease_views()\nadd_comment()\ntranscode_video()</code></pre></div>\n<ol start=\"3\">\n<li><b>Database Schema</b></li>\n</ol>\n<ul>\n<li>We can use <u>relational database for videos and users</u>. We can use <u>non-relational database (like NoSql) for video recommendations, comments</u> etc.</li>\n</ul>\n<ol start=\"4\">\n<li><b>Scalability and Fault Tolerance</b></li>\n</ol>\n<ul>\n<li><b>Service Decomposition</b>: Adopting a microservices architecture enables independent scaling of core components such as video uploads, search, and streaming, ensuring efficient resource utilization based on demand.</li>\n<li><b>Distributed Caching</b>: Implementing caching layers (e.g., Redis) helps store frequently accessed data — such as video metadata, trending content, and user preferences — to deliver faster response times and reduce database load.</li>\n<li><b>Database Sharding</b>: To manage large datasets effectively, databases are partitioned into smaller, distributed shards across multiple servers, improving scalability and the system's ability to handle high data volumes.</li>\n</ul>\n<ol start=\"5\">\n<li><b>Security and Authentication</b></li>\n</ol>\n<ul>\n<li><b>Authentication</b>: Leverage secure protocols like OAuth 2.0 or JSON Web Tokens (JWT) to authenticate users and protect access to the system.</li>\n<li><b>Authorization</b>: Implement Role-Based Access Control (RBAC) to manage user permissions and ensure that only authorized users can perform specific actions.</li>\n<li><b>Data Encryption</b>: Apply end-to-end encryption to safeguard video content and sensitive user data during storage and transmission.</li>\n</ul>\n<p>These represent just a few of the key design elements that must be taken into account when building the architecture of a video streaming platform.</p>\n<hr>\nAuthor: <br/>\n<a href=\"https://www.linkedin.com/in/rahul-majumdar/\">Rahul Majumdar</a>","categories":["System Architecture","System Design"],"date":"February 07, 2025","description":"This article explains the System Design architecture for video streaming application like YouTube.","id":"9f6d462c-bccb-513b-aa6b-3f2a22958369","keywords":["System Design","System Architecture","Video Streaming application","YouTube"],"slug":"/how-to-design-a-scalable-video-streamer/","title":"Do you want to design a video streaming application like YouTube?","readingTime":{"text":"8 min read"}},{"banner":{"alt":"","caption":"","src":null},"body":"<!-- ## Code block test -->\n<!-- \n```css\n.AClass .Subtitle {\n  margin: -0.5rem 0 0 0;\n  font-weight: 700;\n  font-size: 1.25rem;\n  line-height: 1.5rem;\n}\n\n.AnotherClass p {\n  font-size: 1.125rem;\n  margin-bottom: 2rem;\n}\n\n.AThirdClass {\n  display: flex;\n  justify-content: flex-start;\n  align-items: center;\n}\n\n@media (max-width: 768px) {\n  .AClass {\n    flex-direction: column;\n  }\n  .AnotherClass {\n    display: block;\n  }\n}\n``` -->\n<style>\n.styled-table {\n  border-collapse: collapse;\n  margin: 25px 0;\n  font-size: 16px;\n  font-family: sans-serif;\n  min-width: 400px;\n  width: 100%;\n  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n}\n\n.styled-table thead tr {\n  background-color: #009879;\n  color: #ffffff;\n  text-align: left;\n}\n\n.styled-table th,\n.styled-table td {\n  padding: 12px 15px;\n  border: 1px solid #dddddd;\n}\n\n.styled-table tbody tr {\n  border-bottom: 1px solid #dddddd;\n}\n\n.styled-table tbody tr:nth-of-type(even) {\n  background-color: #f3f3f3;\n}\n\n.styled-table tbody tr:hover {\n  background-color: #f1f1f1;\n  cursor: pointer;\n}\n</style>\n<p>Ready to dive into Data Science/Engineering but not sure where to begin? You want to start your new Data Analysis project as soon as possible but do not have the tools to do so yet? You are not sure where and how to start? Well, in this article I explain what you should always have in your local machine as you start on your Data Science journey. This setup will enable you to get started quickly and showcase your progress effectively.</p>\n<h3>Anaconda vs MiniConda: Which Should You Choose?</h3>\n<p>Both Anaconda and Miniconda are distributions of the Conda package and environment management system, but they differ in scope and size.\nAnaconda includes a fully-featured distribution, comes with hundreds of pre-installed data science libraries such as NumPy, pandas, matplotlib, scikit-learn, Jupyter Notebook, and more. You might not need most of them so you can add them later. <br/><br/>\n<u>MiniConda is the way to go for quick setup.</u> It is a minimal installation of Conda, without any pre-installed libraries except Python. You can add only the libraries you need later.</p>\n<table class=\"styled-table\">\n  <tr>\n    <th>Features</th>\n    <th>Anaconda</th>\n    <th>Miniconda</th>\n  </tr>\n  <tr>\n    <td>Size</td>\n    <td>Large installation size (~3 GB)</td>\n    <td>Lightweight (~100 MB), faster to install.</td>\n  </tr>\n  <tr>\n    <td>Pre-installed Libraries</td>\n    <td>Yes (over 250)</td>\n    <td>No (minimal setup)</td>\n  </tr>\n  <tr>\n    <td>Best For</td>\n    <td>Beginners or those who want all tools pre-installed</td>\n    <td>Advanced users and minimalists alike</td>\n  </tr>\n</table>\n<h4>Prerequisite</h4>\n<ul>\n<li>A Windows/Mac/Linux machine</li>\n<li>Internet connection</li>\n<li>Admin rights</li>\n</ul>\n<h4>Installation</h4>\n<p>Go to the Miniconda <u><a href=\"https://www.anaconda.com/docs/getting-started/miniconda/install\" target='_blank'>installation page</a></u> and follow along the steps.</p>\n<h4>Verify your installation</h4>\n<p>Now open the terminal or cmd and check if the installation was completed correctly by adding this command. Verify that you have the latest.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">conda --version</code></pre></div>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 14.545454545454545%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAADCAYAAACTWi8uAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAh0lEQVR42pWM2wrCMBAF+/8/6INiMYUmbSKluWyi0I5JQPDVA8MZlt0dRF9RfkTym5gSKQlJcnWps9I954wLmcu0MinFQ00492S1DrOsaLNQXu/OMN5vqLrUMMYwzzPWWrTW9XnsHmMkhICvnOfJcRy9f/lm2Pcd7wPbttG80Y5bl1IQEf7JB5g85w5aCO8eAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/bff4c2f93c5f8a75fa13ced76a442c85/c9b55/conda_v.webp 165w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/f2908/conda_v.webp 330w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/cc661/conda_v.webp 660w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/a66df/conda_v.webp 990w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/06170/conda_v.webp 1312w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/bff4c2f93c5f8a75fa13ced76a442c85/04c57/conda_v.png 165w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/d9ecf/conda_v.png 330w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/1f083/conda_v.png 660w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/7a3d6/conda_v.png 990w,\n/static/bff4c2f93c5f8a75fa13ced76a442c85/ea964/conda_v.png 1312w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/bff4c2f93c5f8a75fa13ced76a442c85/1f083/conda_v.png\" alt=\"conda v\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span>\n<h4>Now the fun part: creating your developer environments and installing packages</h4>\n<p>On terminal go the folder where you will store your Data Science or Engineering project and type the below commands.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">conda env list\nconda create --name [instert_name] python=3.10</code></pre></div>\n<p>You will notice that there is a base environment. This is where all the base conda packages are installed. However, we do not want to use this environment, instead we want to keep separate sandboxed environments for all our projects. Using the second line we create the new environment. You will be prompted to install the base set of packages, type 'Y' and let the installation get completed.</p>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 123.63636363636363%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAZCAYAAAAxFw7TAAAACXBIWXMAABYlAAAWJQFJUiTwAAACe0lEQVR42p2VWXPiQAyE/f//F2+k2IcANiThxtgcxhfmMGjn00YpdpcQkqnqaAKetlotDV4yDSTMQinLSvK8lCwvJM1yKcqdFMVOUvdZ7mJVHWR/OH6gcjgej7IqDzIMlzIZTySMYvGieCnRIpLJZKIoikJxODiC/d69qNT9+XwWW5fLH9xa3m63c19eFKwsy2S9Xkscx+5FC1mvlrJyyNLEkedSFrkjr/8isfPA42Capu7QSkmm06mMXfpxtJDecC5Nfya/+gvpjjbiT7YSgGkqgwXE/6fp9Xo9AZvNRrbbrcq8vGuq3YGTQuTg/uyPZ43VsXb7+rbkwWAgYDwey3w+V1JqaBFQ/EeXx8EkSVQ20bKsqkqBIXVdP05IBmS4cAbMZjMJw1CzwiwchvTa4YcyhCyKIlkul+pwnufy0+WdTifNCMlkBBngf15Ght8ipF7IA5Bf99Z1fJgQI6gbfcgeYpobkKXtDXx/l5CHqF38XkMOYALAXVqGzNlb/FIytaKxIadNgJXh26ZwiJEzSRiCScgn2uXwLcLX11clwwCkIhMkiZtfP9DvWNT5yxqSxfPzs4xGI5VuzpJZEATSaDQ+CF9eXrTO99z3kMM9yBybIWRHbTns+766jSHcRNb0dwltzKyG7M0UPoMMgn979dM+ZNzMgOsxZG+jSORZu8k/M8ozufZgu92WTqej9eOeJA6HQ61zs9nUl5iSm4RMCbcMCznUDCKcN/BSYqvV0n69K5lCI5HFFJAdpP1+X1018m63qxHCe/OthH13yEYNachGKsQQQQr509OTvL29fdTyJiHNSltYhmTGIerGzwL9yR5i9rQUz33WNr8BU2KLxoMPCcYAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/871e118dd34c1ba258c8d4f243cc4452/c9b55/conda_env1.webp 165w,\n/static/871e118dd34c1ba258c8d4f243cc4452/f2908/conda_env1.webp 330w,\n/static/871e118dd34c1ba258c8d4f243cc4452/cc661/conda_env1.webp 660w,\n/static/871e118dd34c1ba258c8d4f243cc4452/a66df/conda_env1.webp 990w,\n/static/871e118dd34c1ba258c8d4f243cc4452/d33d4/conda_env1.webp 1320w,\n/static/871e118dd34c1ba258c8d4f243cc4452/83d4c/conda_env1.webp 1482w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/871e118dd34c1ba258c8d4f243cc4452/04c57/conda_env1.png 165w,\n/static/871e118dd34c1ba258c8d4f243cc4452/d9ecf/conda_env1.png 330w,\n/static/871e118dd34c1ba258c8d4f243cc4452/1f083/conda_env1.png 660w,\n/static/871e118dd34c1ba258c8d4f243cc4452/7a3d6/conda_env1.png 990w,\n/static/871e118dd34c1ba258c8d4f243cc4452/78958/conda_env1.png 1320w,\n/static/871e118dd34c1ba258c8d4f243cc4452/a83dd/conda_env1.png 1482w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/871e118dd34c1ba258c8d4f243cc4452/1f083/conda_env1.png\" alt=\"conda env1\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span>\n<p>Now switch to the new environement using the code below</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">conda activate </code></pre></div>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 27.272727272727277%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAAA+0lEQVR42o3O2UrDUBSF4bz/Q9SrijggeKmWitWKIJYKItqaDklO0gwnYzMnv6EFb1Rww8deLPbFVqQ+R4QWYZwSRAl+GO+E8ZYo2XdBl7dZQVZU39JOXlYEeYXmBAjTwrJdFMexse0NQhgYukaSxGRZSpHnlGVJnmXdLmjahv+M4klJXTe07b6Qvo+mG6w1jU9VZbXSUBdLTGuDJ31cT+4+rLv7qvlJOR4LTp5CzicJZ88x/TuT3vWCg+Ga3mBJ/1bncKRzdC84fbS5mCZcfrRczfiV8vDuMnqT3LxYDCY6w6lg9Op0nct4FjE3I5y4xgwKrLDES8HZ/u0LnYx15OhTikYAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/9a6a85489e3eab6404487da0dd6ca025/c9b55/conda_a.webp 165w,\n/static/9a6a85489e3eab6404487da0dd6ca025/f2908/conda_a.webp 330w,\n/static/9a6a85489e3eab6404487da0dd6ca025/cc661/conda_a.webp 660w,\n/static/9a6a85489e3eab6404487da0dd6ca025/a66df/conda_a.webp 990w,\n/static/9a6a85489e3eab6404487da0dd6ca025/d33d4/conda_a.webp 1320w,\n/static/9a6a85489e3eab6404487da0dd6ca025/bb4a2/conda_a.webp 1458w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/9a6a85489e3eab6404487da0dd6ca025/04c57/conda_a.png 165w,\n/static/9a6a85489e3eab6404487da0dd6ca025/d9ecf/conda_a.png 330w,\n/static/9a6a85489e3eab6404487da0dd6ca025/1f083/conda_a.png 660w,\n/static/9a6a85489e3eab6404487da0dd6ca025/7a3d6/conda_a.png 990w,\n/static/9a6a85489e3eab6404487da0dd6ca025/78958/conda_a.png 1320w,\n/static/9a6a85489e3eab6404487da0dd6ca025/6b95e/conda_a.png 1458w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/9a6a85489e3eab6404487da0dd6ca025/1f083/conda_a.png\" alt=\"conda a\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span>\n<p>Now that you have created a new Data Science/Engineering environement you can now install packages directly here as shown below.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">conda install numpy pandas jupyter</code></pre></div>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 72.72727272727273%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAABmElEQVR42qVTiW6CQBTc//+4JkZR8MCDioDct4pOd16LIY1NafqSyTzX7DDvWOV4b3CLBHleIc5KJFmBNC8FzIuyRlo0yKsWZdWgqDXXn3lctrDffdjbHTb2Dl4QQpmWgeDsI88yJEmCMAw1x5LHUYTL5YLj8YjNZoP1mlhjtVphaS0lN+ZzeJ6H2+0mUIYxh3M4iFCkBbqukz+u1+sTbXtBVdXPSz26rkeHPtR2u4XjODBNE7Zt46DFKZx9OS6KQotVuiU5Ho8HfgvVti2apnm66plf7R3y7H6/Y0wo9sa0LOz3e/RuXddFXethpKk4LctSPjrKYRzHUiIhuQYFhn35S4hgEAQ4n88iSFfMyXRJZ8RoQfaQF9n0Pic4CIrSLQfDGJb8U/Gqd0Ex7tx/Q/m+j9PpJEsc6V2kKF3RMQcyHAyZ58yJV5NXs9kMi8UC0+lU2DAMmTKF6Z4XWT6rIA9b8lKQ051MJrLE3Df2i0P63rPRJfOd0qXv+fL8WD75lSB/D/FSMMvS5zOjEHtGl2S65xmdj3X4AUB8ihD6yH0CAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/80e436ab8daba965b174823d878ece2c/c9b55/conda_j.webp 165w,\n/static/80e436ab8daba965b174823d878ece2c/f2908/conda_j.webp 330w,\n/static/80e436ab8daba965b174823d878ece2c/cc661/conda_j.webp 660w,\n/static/80e436ab8daba965b174823d878ece2c/a66df/conda_j.webp 990w,\n/static/80e436ab8daba965b174823d878ece2c/41bb6/conda_j.webp 1222w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/80e436ab8daba965b174823d878ece2c/04c57/conda_j.png 165w,\n/static/80e436ab8daba965b174823d878ece2c/d9ecf/conda_j.png 330w,\n/static/80e436ab8daba965b174823d878ece2c/1f083/conda_j.png 660w,\n/static/80e436ab8daba965b174823d878ece2c/7a3d6/conda_j.png 990w,\n/static/80e436ab8daba965b174823d878ece2c/636d3/conda_j.png 1222w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/80e436ab8daba965b174823d878ece2c/1f083/conda_j.png\" alt=\"conda j\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span>\nAs shown, only the 3 packages will be installed and only in this new environment and folder. \n<u>This way all your data projects are sandboxed, only have lightweight packages that you would actually use and you maintain data integrity.</u>\n<h4>Now let's run Jupyter</h4>\n<p>Now we will open <u>Jupyter</u> and see if our development environment is good to go. Ensure that you are in the desired Conda environment and type the below command in the terminal:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">jupyter notebook</code></pre></div>\n<p>You will see initialization steps as below and soon you will see the browser open <u>Jupyter Notebook</u>.\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 43.03030303030303%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAABdElEQVR42o1Si26DMBDj//8PyiOPAYWy8QghhbZA6+Wu66Rp0rRI1l2A2D6HoOxL9HZAN3YYqBoLWRvoesByueDiMV+uaHuDtm1hzIhhMBjH0cNiXTfs+/2J+x2BmgT0JJE7Be0EUp0hSRWSTKAoCmilGES27zvuHn+tQJ5T6FlCnwWoT0QC6cmEyJCmfp8kiOPY7wWstZjn2bu64XZ7Yl1XFmIxciidJzx7h4uGNJ4kS5mICMhhXddcj8cju+y6jsc1xnA/DIMXOX8JbQj0IvE2a5RLzk5VoSC8Qyl970clEBkdJofX6/XbHTn6NXJtKrzbE1r3gWaq0ZueXTVNw/V0OnGtqgplWTI59Xmes8jj8fiBoKkaVEXFKN9KJqLRXnlN08SgrP6zAnWU/mYTRFmIWB1wiA+IwghRFPFl0OhhGPLlUHYkRLlRhi8x6umdcw6BdRajNTAebnb8wesg5bVtG1dySJX+y2VZ2D3tCdTTM8r1E3NyqFhEHy6/AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/45249c30085c6f375b83dc5bcefeeb01/c9b55/conda_j2.webp 165w,\n/static/45249c30085c6f375b83dc5bcefeeb01/f2908/conda_j2.webp 330w,\n/static/45249c30085c6f375b83dc5bcefeeb01/cc661/conda_j2.webp 660w,\n/static/45249c30085c6f375b83dc5bcefeeb01/a66df/conda_j2.webp 990w,\n/static/45249c30085c6f375b83dc5bcefeeb01/d33d4/conda_j2.webp 1320w,\n/static/45249c30085c6f375b83dc5bcefeeb01/2be49/conda_j2.webp 2180w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/45249c30085c6f375b83dc5bcefeeb01/04c57/conda_j2.png 165w,\n/static/45249c30085c6f375b83dc5bcefeeb01/d9ecf/conda_j2.png 330w,\n/static/45249c30085c6f375b83dc5bcefeeb01/1f083/conda_j2.png 660w,\n/static/45249c30085c6f375b83dc5bcefeeb01/7a3d6/conda_j2.png 990w,\n/static/45249c30085c6f375b83dc5bcefeeb01/78958/conda_j2.png 1320w,\n/static/45249c30085c6f375b83dc5bcefeeb01/1078c/conda_j2.png 2180w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/45249c30085c6f375b83dc5bcefeeb01/1f083/conda_j2.png\" alt=\"conda j2\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 660px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 25.454545454545457%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAh0lEQVR42p2QyQ7DIAxE+f+/JDfWUHYRYIqR2lukNpae5MPYnjErOcEaA3ueuK4Lc85H9N43LPoXlJKQUsI5h5TSI3LOGyaVgtYanHMIIXAupyEExBh/grRkhOYIZq3dm8lua23H/qcoLs0QtVYws/53HAe891/BGOMWOvzpSUuvUislVSkFb7vlhX+3bAwTAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"></span>\n  <picture>\n          <source srcset=\"/static/742a45b2c6e5c394bff916428bc5d317/c9b55/jypyter_i.webp 165w,\n/static/742a45b2c6e5c394bff916428bc5d317/f2908/jypyter_i.webp 330w,\n/static/742a45b2c6e5c394bff916428bc5d317/cc661/jypyter_i.webp 660w,\n/static/742a45b2c6e5c394bff916428bc5d317/a66df/jypyter_i.webp 990w,\n/static/742a45b2c6e5c394bff916428bc5d317/d33d4/jypyter_i.webp 1320w,\n/static/742a45b2c6e5c394bff916428bc5d317/57b3a/jypyter_i.webp 2376w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/webp\">\n          <source srcset=\"/static/742a45b2c6e5c394bff916428bc5d317/04c57/jypyter_i.png 165w,\n/static/742a45b2c6e5c394bff916428bc5d317/d9ecf/jypyter_i.png 330w,\n/static/742a45b2c6e5c394bff916428bc5d317/1f083/jypyter_i.png 660w,\n/static/742a45b2c6e5c394bff916428bc5d317/7a3d6/jypyter_i.png 990w,\n/static/742a45b2c6e5c394bff916428bc5d317/78958/jypyter_i.png 1320w,\n/static/742a45b2c6e5c394bff916428bc5d317/59058/jypyter_i.png 2376w\" sizes=\"(max-width: 660px) 100vw, 660px\" type=\"image/png\">\n          <img class=\"gatsby-resp-image-image\" src=\"/static/742a45b2c6e5c394bff916428bc5d317/1f083/jypyter_i.png\" alt=\"jypyter i\" title=\"\" loading=\"lazy\" decoding=\"async\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\">\n        </picture>\n    </span></p>\n<p>Now you can upload data files using the Jupyter web interface, or can copy over the files to the env folder.</p>\n<h4>Summary</h4>\n<p>You are now all set. You have setup environments, installed packages, and installed Jupyter locally.\nYou can create a new notebook and start your Data Science &#x26; Engineering journey.</p>\n<hr>\nAuthor: <br/>\n<u><a href=\"https://www.linkedin.com/in/rahul-majumdar/\" target='_blank'>Rahul Majumdar</a></u>","categories":["DataEngineering","DataScience","Jupyter","Python"],"date":"February 01, 2025","description":"Below are the steps quickly setup and start your Data Science journey/project from Day 0.","id":"01f86f0a-662f-5cda-84ff-fe445c202cfb","keywords":["Data Science","Conda","Jupyter","Python","Setup"],"slug":"/fastest-way-to-setup-datascience-environment-in-local-machine/","title":"Kickstart Data Science Locally: From Zero to Jupyter in 5 Steps","readingTime":{"text":"4 min read"}},{"banner":{"alt":"","caption":"","src":null},"body":"<style>\n.styled-table {\n  border-collapse: collapse;\n  margin: 25px 0;\n  font-size: 16px;\n  font-family: sans-serif;\n  min-width: 400px;\n  width: 100%;\n  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n}\n\n.styled-table thead tr {\n  background-color: #009879;\n  color: #ffffff;\n  text-align: left;\n}\n\n.styled-table th,\n.styled-table td {\n  padding: 12px 15px;\n  border: 1px solid #dddddd;\n}\n\n.styled-table tbody tr {\n  border-bottom: 1px solid #dddddd;\n}\n\n.styled-table tbody tr:nth-of-type(even) {\n  background-color: #f3f3f3;\n}\n\n.styled-table tbody tr:hover {\n  background-color: #f1f1f1;\n  cursor: pointer;\n}\n\n.gatsby-resp-image-wrapper {\n  display: inline !important;\n}\n\n</style>\n<h2>Building AI Applications with a Model Context Protocol (MCP) Server</h2>\n<p>Large Language Models (LLMs) like GPT-4 or Claude are powerful but limited by their <strong>context window</strong> - the amount of information they can process at once. Real-world AI applications often need external data (e.g., user profiles, documents, system states) to respond intelligently and accurately. This is where a <strong>Model Context Protocol (MCP) server</strong> comes in.</p>\n<p>An MCP server acts as a <strong>context manager</strong> for LLMs, delivering dynamic, relevant information during inference. In this blog, we’ll explore <strong>what an MCP server is, how it works, and how it can be used to build context-aware AI applications.</strong></p>\n<hr>\n<h2>What is a Model Context Protocol (MCP) Server?</h2>\n<p>A <strong>Model Context Protocol server</strong> is a specialized service designed to:</p>\n<ul>\n<li><strong>Store, retrieve, and serve context data</strong> to LLMs during inference</li>\n<li><strong>Bridge the gap between stateless LLMs and dynamic application data</strong></li>\n<li>Enable <strong>context injection</strong>, ensuring responses are grounded in relevant knowledge</li>\n</ul>\n<p>Think of MCP as an <strong>\"AI middleware\"</strong> that fetches, preprocesses, and streams context to the LLM before generating responses.</p>\n<hr>\n<h2>Why Do We Need MCP?</h2>\n<ul>\n<li><strong>Context Window Limitations:</strong> LLMs have fixed token limits. MCP helps by managing external memory.</li>\n<li><strong>Personalization:</strong> Store user-specific preferences, past interactions, or domain-specific data.</li>\n<li><strong>Dynamic Knowledge:</strong> Connect to real-time data sources (e.g., healthcare records, financial data).</li>\n<li><strong>RAG (Retrieval-Augmented Generation):</strong> Combine MCP with vector search to ground responses in factual information.</li>\n</ul>\n<hr>\n<h2>MCP Server Architecture</h2>\n<p>An MCP server typically has these components:</p>\n<ol>\n<li>\n<p><strong>Context Storage Layer</strong><br>\nStores user sessions, embeddings, or structured data (e.g., Redis, PostgreSQL, or a vector database like Pinecone).</p>\n</li>\n<li>\n<p><strong>Retrieval Engine</strong><br>\nUses semantic search (via embeddings) or query filters to fetch the most relevant context for each request.</p>\n</li>\n<li>\n<p><strong>Preprocessing Layer</strong><br>\nCleans, summarizes, or chunks retrieved data to fit the LLM’s input size.</p>\n</li>\n<li>\n<p><strong>Inference Orchestration</strong><br>\nMerges the <strong>prompt template</strong>, user query, and context before sending to the LLM.</p>\n</li>\n</ol>\n<hr>\n<h3>Example Workflow</h3>\n<div class=\"gatsby-highlight\" data-language=\"plaintext\"><pre class=\"language-plaintext\"><code class=\"language-plaintext\">User Query\n   ↓\nMCP Server\n   ├── Retrieve context (from DB, APIs, or documents)\n   ├── Preprocess (summarize or chunk data)\n   └── Send combined prompt to LLM\n        ↓\n   Return AI-generated response to application</code></pre></div>\n<h2>Building an AI App with MCP</h2>\n<p>Here’s a high-level guide on how to use MCP in an AI application:</p>\n<p>Step 1: Set Up Context Storage\nUse a vector database for semantic search (e.g., Pinecone, Weaviate, or FAISS):</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> sentence_transformers <span class=\"token keyword\">import</span> SentenceTransformer\n<span class=\"token keyword\">import</span> faiss\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\nmodel <span class=\"token operator\">=</span> SentenceTransformer<span class=\"token punctuation\">(</span><span class=\"token string\">'all-MiniLM-L6-v2'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Example documents</span>\ndocs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"Patient has diabetes type 2.\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Recommended medication: Metformin.\"</span><span class=\"token punctuation\">]</span>\nembeddings <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>model<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>d<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> d <span class=\"token keyword\">in</span> docs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nindex <span class=\"token operator\">=</span> faiss<span class=\"token punctuation\">.</span>IndexFlatL2<span class=\"token punctuation\">(</span>embeddings<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nindex<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>embeddings<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Step 2: Create an MCP Retrieval API\nThis API will query the vector store to retrieve context:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> flask <span class=\"token keyword\">import</span> Flask<span class=\"token punctuation\">,</span> request<span class=\"token punctuation\">,</span> jsonify\n\napp <span class=\"token operator\">=</span> Flask<span class=\"token punctuation\">(</span>__name__<span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@app<span class=\"token punctuation\">.</span>route</span><span class=\"token punctuation\">(</span><span class=\"token string\">'/retrieve'</span><span class=\"token punctuation\">,</span> methods<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'POST'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">retrieve_context</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    query <span class=\"token operator\">=</span> request<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">[</span><span class=\"token string\">'query'</span><span class=\"token punctuation\">]</span>\n    query_embedding <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n    D<span class=\"token punctuation\">,</span> I <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span>search<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>query_embedding<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> k<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    results <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>docs<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> I<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> jsonify<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"context\"</span><span class=\"token punctuation\">:</span> results<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Step 3: Merge Context with User Query\nYour application merges retrieved context with a prompt template before calling the LLM:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">import</span> openai\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">generate_response</span><span class=\"token punctuation\">(</span>user_query<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    context <span class=\"token operator\">=</span> get_context_from_mcp<span class=\"token punctuation\">(</span>user_query<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># call MCP /retrieve</span>\n    prompt <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"Context:\\n</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>context<span class=\"token punctuation\">}</span></span><span class=\"token string\">\\n\\nUser Query: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>user_query<span class=\"token punctuation\">}</span></span><span class=\"token string\">\\nAnswer:\"</span></span>\n    response <span class=\"token operator\">=</span> openai<span class=\"token punctuation\">.</span>ChatCompletion<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4\"</span><span class=\"token punctuation\">,</span>\n        messages<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"user\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> prompt<span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> response<span class=\"token punctuation\">[</span><span class=\"token string\">'choices'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'message'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'content'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>Step 4: Deploy MCP Server</p>\n<ul>\n<li>Deploy MCP as a REST or gRPC microservice.</li>\n<li>Integrate it with your AI application stack (e.g., FastAPI backend or Node.js service).</li>\n<li>Add caching (e.g., Redis) to improve response time for frequent queries.</li>\n</ul>\n<h2>Real-World Use Cases</h2>\n<ul>\n<li>Healthcare Assistant</li>\n<li>MCP retrieves patient history, lab results, and clinical guidelines to help doctors with quick recommendations.</li>\n<li>Customer Support Chatbots</li>\n<li>Knowledge Workers - Integrate MCP with document repositories and use RAG to summarize company policies or research papers.</li>\n</ul>\n<h2>Benefits of Using MCP with LLMs</h2>\n<ul>\n<li>Scalable Context Management: Offloads memory management from the LLM.</li>\n<li>Improved Accuracy: Grounded responses based on real-time, relevant data.</li>\n<li>Seamless Personalization: Maintain user-specific context without retraining.</li>\n<li>Interoperability: MCP can integrate with any LLM API (OpenAI, Anthropic, AWS Bedrock, etc.).</li>\n</ul>\n<h2>Further reading</h2>\n<ul>\n<li><u><a href='https://arxiv.org/abs/2005.11401'>Retrieval-Augmented Generation (RAG)</a></u></li>\n<li><u><a href='https://platform.openai.com/docs/overview'>OpenAI API Docs</a></u></li>\n<li><u><a href=\"https://www.pinecone.io/\">Pinecone Vector Database</a></u></li>\n<li><u><a href='https://github.com/facebookresearch/faiss'>FAISS (Facebook AI Similarity Search)</a></u></li>\n</ul>\n<hr>\nAuthor: <br/>\n<a href=\"https://www.linkedin.com/in/rahul-majumdar/\">Rahul Majumdar</a>","categories":["LLM","MCP","AI systems"],"date":"January 15, 2025","description":"This article explains how to build AI Applications with MCP Server","id":"fb5520e5-7083-58ce-97e7-31b817b40789","keywords":["Artificial Intelligence","System Design","System Architecture"],"slug":"/building-ai-applications-with-mcp-server/","title":"Beyond the Context Window: How MCP Makes LLMs Smarter","readingTime":{"text":"4 min read"}},{"banner":{"alt":"","caption":"","src":null},"body":"<h2>Building AI Applications with AWS Bedrock</h2>\n<p>In the fast-evolving world of AI, developers are seeking scalable, low-latency solutions to integrate foundation models into their applications without the complexity of managing infrastructure. <strong>AWS Bedrock</strong> is Amazon’s fully managed service that enables you to build and scale generative AI applications using foundation models from multiple leading AI providers - all through a simple API.</p>\n<p>In this blog, we’ll explore what AWS Bedrock is, how it works, and how to use it to power AI-driven applications.</p>\n<hr>\n<h2>What is AWS Bedrock?</h2>\n<p><strong>AWS Bedrock</strong> is a <strong>serverless service</strong> that provides access to <strong>foundation models (FMs)</strong> from top AI companies - such as <strong>Anthropic (Claude)</strong>, <strong>Meta (Llama)</strong>, <strong>Mistral</strong>, <strong>Cohere</strong>, and <strong>Amazon Titan</strong> - via a unified API. Bedrock allows you to easily experiment with and integrate these models into your applications without having to manage GPU infrastructure, install frameworks, or handle scaling.</p>\n<hr>\n<h2>Key Features</h2>\n<ul>\n<li><strong>Multi-model support</strong>: Choose from top models (Claude, Llama, Mistral, etc.)</li>\n<li><strong>No infrastructure to manage</strong>: Fully serverless with on-demand scalability</li>\n<li><strong>Unified API</strong>: One API to call multiple foundation models</li>\n<li><strong>Customization</strong>: Fine-tune or ground models with your own data using <strong>Retrieval-Augmented Generation (RAG)</strong> and <strong>model fine-tuning</strong></li>\n<li><strong>Enterprise-ready</strong>: Secure, private, and integrated with other AWS services</li>\n</ul>\n<hr>\n<h2>How AWS Bedrock Works</h2>\n<p>Here's a simplified workflow for using Bedrock:</p>\n<div class=\"gatsby-highlight\" data-language=\"plaintext\"><pre class=\"language-plaintext\"><code class=\"language-plaintext\">Client Application\n     ↓\nAWS SDK / Bedrock API\n     ↓\nChoose FM Provider &amp; Model\n     ↓\nSend Prompt (text, image, etc.)\n     ↓\nFoundation Model (Claude, Llama, Titan, etc.)\n     ↓\nReceive Generated Output</code></pre></div>\n<p>You can also integrate Amazon RAG, Amazon Kendra, or vector stores for context-aware generation using your proprietary data.</p>\n<h2>Example: Using AWS Bedrock to Generate Text with Claude</h2>\n<p>Step 1: Set up your AWS CLI and permissions\nMake sure you have bedrock:InvokeModel permission and have enabled Bedrock access in your region.</p>\n<p>Step 2: Invoke a model (e.g., Claude) using the AWS SDK for Python (Boto3)</p>\n<div class=\"gatsby-highlight\" data-language=\"js\"><pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">import</span> boto3\n<span class=\"token keyword\">import</span> json\n\nclient <span class=\"token operator\">=</span> boto3<span class=\"token punctuation\">.</span><span class=\"token function\">client</span><span class=\"token punctuation\">(</span><span class=\"token string\">'bedrock-runtime'</span><span class=\"token punctuation\">)</span>\n\nresponse <span class=\"token operator\">=</span> client<span class=\"token punctuation\">.</span><span class=\"token function\">invoke_model</span><span class=\"token punctuation\">(</span>\n    modelId<span class=\"token operator\">=</span><span class=\"token string\">'anthropic.claude-3-sonnet-20240229-v1:0'</span><span class=\"token punctuation\">,</span>\n    body<span class=\"token operator\">=</span>json<span class=\"token punctuation\">.</span><span class=\"token function\">dumps</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n        <span class=\"token string-property property\">\"messages\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string-property property\">\"role\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"user\"</span><span class=\"token punctuation\">,</span> <span class=\"token string-property property\">\"content\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Summarize AWS Bedrock in two lines\"</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string-property property\">\"max_tokens\"</span><span class=\"token operator\">:</span> <span class=\"token number\">200</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    contentType<span class=\"token operator\">=</span><span class=\"token string\">'application/json'</span><span class=\"token punctuation\">,</span>\n    accept<span class=\"token operator\">=</span><span class=\"token string\">'application/json'</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token function\">print</span><span class=\"token punctuation\">(</span>response<span class=\"token punctuation\">[</span><span class=\"token string\">'body'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span><span class=\"token function\">read</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token function\">decode</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<hr>\n<h2>AWS Bedrock Use Cases (as of January 2025)</h2>\n<ul>\n<li>Chatbots and Virtual Assistants - Use Claude or Titan to build intelligent customer support agents.</li>\n<li>Document Summarization - Generate concise summaries of long documents for knowledge workers.</li>\n<li>Code Generation - Integrate Cohere or Meta models for AI-assisted coding features.</li>\n<li>Search and Retrieval - Combine RAG with Amazon Kendra to build AI-powered enterprise search.</li>\n</ul>\n<h2>Security and Governance</h2>\n<ul>\n<li>Private model execution: No data leaves the VPC or gets used to train the models.</li>\n<li>Access control: Use IAM to define permissions.</li>\n<li>Monitoring: Integrate with CloudWatch and AWS CloudTrail for auditing and monitoring.</li>\n</ul>\n<hr>\n<h2>Retrieval-Augmented Generation (RAG)</h2>\n<p>Combine foundation models with your proprietary data:</p>\n<ul>\n<li>Store documents in Amazon S3</li>\n<li>Use vector databases like Amazon OpenSearch or Pinecone</li>\n<li>Use Amazon Kendra to extract relevant context</li>\n<li>Pass that context along with the prompt to Bedrock</li>\n</ul>\n<h2>Integration with Other AWS Services</h2>\n<ul>\n<li><b>Amazon SageMaker</b>: Bring-your-own-model workflows</li>\n<li><b>AWS Lambda</b>: Create event-driven AI functions</li>\n<li><b>API Gateway + Bedrock</b>: Build serverless AI APIs</li>\n<li><b>Step Functions</b>: Orchestrate multi-step AI pipelines</li>\n<li><b>S3 / DynamoDB</b>: Store input and output data</li>\n</ul>\n<h2>Further deep-dive</h2>\n<ul>\n<li><u><a href='https://docs.aws.amazon.com/bedrock/'>AWS Bedrock Documentation</a></u></li>\n<li><u><a href='https://github.com/aws-samples/amazon-bedrock-samples'>AWS Bedrock GitHub Samples</a></u></li>\n</ul>\n<hr>\nAuthor: <br/>\n<u><a href=\"https://www.linkedin.com/in/rahul-majumdar/\">Rahul Majumdar</a></u>","categories":["AWS","AI systems"],"date":"January 01, 2025","description":"Generative AI at Scale: A Developer’s Guide to AWS Bedrock","id":"eb3567a0-eca4-5f9a-bb06-af1184304c5c","keywords":["Artificial Intelligence","System Design","AWS"],"slug":"/build-ai-applications-using-aws-bedrock/","title":"Generative AI at Scale: A Developer’s Guide to AWS Bedrock","readingTime":{"text":"3 min read"}}]}},"staticQueryHashes":["3262260831","948380417"],"slicesMap":{}}