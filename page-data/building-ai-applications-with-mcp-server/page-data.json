{"componentChunkName":"component---node-modules-gatsby-theme-portfolio-minimal-src-templates-article-index-tsx","path":"/building-ai-applications-with-mcp-server/","result":{"pageContext":{"article":{"banner":{"alt":"","caption":"","src":null},"body":"<style>\n.styled-table {\n  border-collapse: collapse;\n  margin: 25px 0;\n  font-size: 16px;\n  font-family: sans-serif;\n  min-width: 400px;\n  width: 100%;\n  box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n}\n\n.styled-table thead tr {\n  background-color: #009879;\n  color: #ffffff;\n  text-align: left;\n}\n\n.styled-table th,\n.styled-table td {\n  padding: 12px 15px;\n  border: 1px solid #dddddd;\n}\n\n.styled-table tbody tr {\n  border-bottom: 1px solid #dddddd;\n}\n\n.styled-table tbody tr:nth-of-type(even) {\n  background-color: #f3f3f3;\n}\n\n.styled-table tbody tr:hover {\n  background-color: #f1f1f1;\n  cursor: pointer;\n}\n\n.gatsby-resp-image-wrapper {\n  display: inline !important;\n}\n\n</style>\n<h2>Building AI Applications with a Model Context Protocol (MCP) Server</h2>\n<p>Large Language Models (LLMs) like GPT-4 or Claude are powerful but limited by their <strong>context window</strong> - the amount of information they can process at once. Real-world AI applications often need external data (e.g., user profiles, documents, system states) to respond intelligently and accurately. This is where a <strong>Model Context Protocol (MCP) server</strong> comes in.</p>\n<p>An MCP server acts as a <strong>context manager</strong> for LLMs, delivering dynamic, relevant information during inference. In this blog, we’ll explore <strong>what an MCP server is, how it works, and how it can be used to build context-aware AI applications.</strong></p>\n<hr>\n<h2>What is a Model Context Protocol (MCP) Server?</h2>\n<p>A <strong>Model Context Protocol server</strong> is a specialized service designed to:</p>\n<ul>\n<li><strong>Store, retrieve, and serve context data</strong> to LLMs during inference</li>\n<li><strong>Bridge the gap between stateless LLMs and dynamic application data</strong></li>\n<li>Enable <strong>context injection</strong>, ensuring responses are grounded in relevant knowledge</li>\n</ul>\n<p>Think of MCP as an <strong>\"AI middleware\"</strong> that fetches, preprocesses, and streams context to the LLM before generating responses.</p>\n<hr>\n<h2>Why Do We Need MCP?</h2>\n<ul>\n<li><strong>Context Window Limitations:</strong> LLMs have fixed token limits. MCP helps by managing external memory.</li>\n<li><strong>Personalization:</strong> Store user-specific preferences, past interactions, or domain-specific data.</li>\n<li><strong>Dynamic Knowledge:</strong> Connect to real-time data sources (e.g., healthcare records, financial data).</li>\n<li><strong>RAG (Retrieval-Augmented Generation):</strong> Combine MCP with vector search to ground responses in factual information.</li>\n</ul>\n<hr>\n<h2>MCP Server Architecture</h2>\n<p>An MCP server typically has these components:</p>\n<ol>\n<li>\n<p><strong>Context Storage Layer</strong><br>\nStores user sessions, embeddings, or structured data (e.g., Redis, PostgreSQL, or a vector database like Pinecone).</p>\n</li>\n<li>\n<p><strong>Retrieval Engine</strong><br>\nUses semantic search (via embeddings) or query filters to fetch the most relevant context for each request.</p>\n</li>\n<li>\n<p><strong>Preprocessing Layer</strong><br>\nCleans, summarizes, or chunks retrieved data to fit the LLM’s input size.</p>\n</li>\n<li>\n<p><strong>Inference Orchestration</strong><br>\nMerges the <strong>prompt template</strong>, user query, and context before sending to the LLM.</p>\n</li>\n</ol>\n<hr>\n<h3>Example Workflow</h3>\n<div class=\"gatsby-highlight\" data-language=\"plaintext\"><pre class=\"language-plaintext\"><code class=\"language-plaintext\">User Query\n   ↓\nMCP Server\n   ├── Retrieve context (from DB, APIs, or documents)\n   ├── Preprocess (summarize or chunk data)\n   └── Send combined prompt to LLM\n        ↓\n   Return AI-generated response to application</code></pre></div>\n<h2>Building an AI App with MCP</h2>\n<p>Here’s a high-level guide on how to use MCP in an AI application:</p>\n<p>Step 1: Set Up Context Storage\nUse a vector database for semantic search (e.g., Pinecone, Weaviate, or FAISS):</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> sentence_transformers <span class=\"token keyword\">import</span> SentenceTransformer\n<span class=\"token keyword\">import</span> faiss\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\nmodel <span class=\"token operator\">=</span> SentenceTransformer<span class=\"token punctuation\">(</span><span class=\"token string\">'all-MiniLM-L6-v2'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Example documents</span>\ndocs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"Patient has diabetes type 2.\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"Recommended medication: Metformin.\"</span><span class=\"token punctuation\">]</span>\nembeddings <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>model<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>d<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> d <span class=\"token keyword\">in</span> docs<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nindex <span class=\"token operator\">=</span> faiss<span class=\"token punctuation\">.</span>IndexFlatL2<span class=\"token punctuation\">(</span>embeddings<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nindex<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>embeddings<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Step 2: Create an MCP Retrieval API\nThis API will query the vector store to retrieve context:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">from</span> flask <span class=\"token keyword\">import</span> Flask<span class=\"token punctuation\">,</span> request<span class=\"token punctuation\">,</span> jsonify\n\napp <span class=\"token operator\">=</span> Flask<span class=\"token punctuation\">(</span>__name__<span class=\"token punctuation\">)</span>\n\n<span class=\"token decorator annotation punctuation\">@app<span class=\"token punctuation\">.</span>route</span><span class=\"token punctuation\">(</span><span class=\"token string\">'/retrieve'</span><span class=\"token punctuation\">,</span> methods<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'POST'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">retrieve_context</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    query <span class=\"token operator\">=</span> request<span class=\"token punctuation\">.</span>json<span class=\"token punctuation\">[</span><span class=\"token string\">'query'</span><span class=\"token punctuation\">]</span>\n    query_embedding <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n    D<span class=\"token punctuation\">,</span> I <span class=\"token operator\">=</span> index<span class=\"token punctuation\">.</span>search<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>query_embedding<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> k<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    results <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>docs<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> I<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> jsonify<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"context\"</span><span class=\"token punctuation\">:</span> results<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Step 3: Merge Context with User Query\nYour application merges retrieved context with a prompt template before calling the LLM:</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">import</span> openai\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">generate_response</span><span class=\"token punctuation\">(</span>user_query<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    context <span class=\"token operator\">=</span> get_context_from_mcp<span class=\"token punctuation\">(</span>user_query<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># call MCP /retrieve</span>\n    prompt <span class=\"token operator\">=</span> <span class=\"token string-interpolation\"><span class=\"token string\">f\"Context:\\n</span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>context<span class=\"token punctuation\">}</span></span><span class=\"token string\">\\n\\nUser Query: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>user_query<span class=\"token punctuation\">}</span></span><span class=\"token string\">\\nAnswer:\"</span></span>\n    response <span class=\"token operator\">=</span> openai<span class=\"token punctuation\">.</span>ChatCompletion<span class=\"token punctuation\">.</span>create<span class=\"token punctuation\">(</span>\n        model<span class=\"token operator\">=</span><span class=\"token string\">\"gpt-4\"</span><span class=\"token punctuation\">,</span>\n        messages<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">{</span><span class=\"token string\">\"role\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"user\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"content\"</span><span class=\"token punctuation\">:</span> prompt<span class=\"token punctuation\">}</span><span class=\"token punctuation\">]</span>\n    <span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> response<span class=\"token punctuation\">[</span><span class=\"token string\">'choices'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'message'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token string\">'content'</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>Step 4: Deploy MCP Server</p>\n<ul>\n<li>Deploy MCP as a REST or gRPC microservice.</li>\n<li>Integrate it with your AI application stack (e.g., FastAPI backend or Node.js service).</li>\n<li>Add caching (e.g., Redis) to improve response time for frequent queries.</li>\n</ul>\n<h2>Real-World Use Cases</h2>\n<ul>\n<li>Healthcare Assistant</li>\n<li>MCP retrieves patient history, lab results, and clinical guidelines to help doctors with quick recommendations.</li>\n<li>Customer Support Chatbots</li>\n<li>Knowledge Workers - Integrate MCP with document repositories and use RAG to summarize company policies or research papers.</li>\n</ul>\n<h2>Benefits of Using MCP with LLMs</h2>\n<ul>\n<li>Scalable Context Management: Offloads memory management from the LLM.</li>\n<li>Improved Accuracy: Grounded responses based on real-time, relevant data.</li>\n<li>Seamless Personalization: Maintain user-specific context without retraining.</li>\n<li>Interoperability: MCP can integrate with any LLM API (OpenAI, Anthropic, AWS Bedrock, etc.).</li>\n</ul>\n<h2>Further reading</h2>\n<ul>\n<li><u><a href='https://arxiv.org/abs/2005.11401'>Retrieval-Augmented Generation (RAG)</a></u></li>\n<li><u><a href='https://platform.openai.com/docs/overview'>OpenAI API Docs</a></u></li>\n<li><u><a href=\"https://www.pinecone.io/\">Pinecone Vector Database</a></u></li>\n<li><u><a href='https://github.com/facebookresearch/faiss'>FAISS (Facebook AI Similarity Search)</a></u></li>\n</ul>\n<hr>\nAuthor: <br/>\n<a href=\"https://www.linkedin.com/in/rahul-majumdar/\">Rahul Majumdar</a>","categories":["LLM","MCP","AI systems"],"date":"January 15, 2025","description":"This article explains how to build AI Applications with MCP Server","id":"fb5520e5-7083-58ce-97e7-31b817b40789","keywords":["Artificial Intelligence","System Design","System Architecture"],"slug":"/building-ai-applications-with-mcp-server/","title":"Beyond the Context Window: How MCP Makes LLMs Smarter","readingTime":{"text":"4 min read"}},"listingPagePath":"/blog"}},"staticQueryHashes":["3262260831","948380417"],"slicesMap":{}}